{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:solid Chocolate 2px; padding: 40px\">\n",
    "\n",
    "–ü—Ä–∏–≤–µ—Ç, –º–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ö—É–∏–º–æ–≤. –Ø –±—É–¥—É —Ä–µ–≤—å—é–µ—Ä–æ–º —Ç–≤–æ–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. –¢—ã –º–æ–∂–µ—à—å –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫–æ –º–Ω–µ –Ω–∞ \"—Ç—ã\"üòè –ù–∞–¥–µ—é—Å—å, —Ç–µ–±—è —Ç–∞–∫–∂–µ –Ω–µ —Å–º—É—Ç–∏—Ç, –µ—Å–ª–∏ —è –±—É–¥—É –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ —Ç–µ–±–µ –Ω–∞ \"—Ç—ã\", –Ω–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ—É–¥–æ–±–Ω–æ, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º!\n",
    "\n",
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–µ —É–¥–∞–ª—è–π –º–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –æ–Ω–∏ –±—É–¥—É—Ç –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –Ω–∞—à–µ–π —Ä–∞–±–æ—Ç—ã –≤ —Å–ª—É—á–∞–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞. \n",
    "\n",
    "–¢—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—à—å —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –º–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å–≤–æ–∏–º–∏ –ø–æ —à–∞–±–ª–æ–Ω—É, –ø–æ–∫–∞–∑–∞–Ω–Ω–æ–º—É —á—É—Ç—å –Ω–∏–∂–µ. –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞–ª–∞—Å—å –ø—É—Ç–∞–Ω–∏—Ü–∞üòâ\n",
    "\n",
    "–¢—ã –º–æ–∂–µ—à—å –Ω–∞–π—Ç–∏ –º–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –æ–±–æ–∑–Ω–∞—á–µ–Ω–Ω—ã–µ <font color='green'>–∑–µ–ª–µ–Ω—ã–º</font>, <font color='gold'>–∂–µ–ª—Ç—ã–º</font> –∏ <font color='red'>–∫—Ä–∞—Å–Ω—ã–º</font> —Ü–≤–µ—Ç–∞–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b> –í —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —à–∞–≥–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>\n",
    "    \n",
    "<b>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏üí°:</b> –í —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —à–∞–≥–µ —Å—Ç–∞–Ω–µ—Ç –µ—â–µ –ª—É—á—à–µ, –µ—Å–ª–∏ –≤–Ω–µ—Å—Ç–∏ –Ω–µ–±–æ–ª—å—à–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏–≤—ã.\n",
    "</div>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"></h2>\n",
    "\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b>\n",
    " –í —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —à–∞–≥–µ —Ç—Ä–µ–±—É–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏ –∏ –≤–Ω–µ—Å–µ–Ω–∏—è –ø—Ä–∞–≤–æ–∫. –ù–∞–ø–æ–º–∏–Ω–∞—é, —á—Ç–æ –ø—Ä–æ–µ–∫—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–Ω—è—Ç —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑–∞, –µ—Å–ª–∏ —Ä–µ–≤—å—é —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—â–∏–µ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å —à–∞–≥–∏.\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<br/>    \n",
    "<div class=\"alert alert-info\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞: <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>üëã:</b> –í —Ç–∞–∫–æ–π —Ü–≤–µ—Ç–æ–≤–æ–π —è—á–µ–π–∫–µ —è –ø—Ä–æ—à—É —Ç–µ–±—è –æ—Å—Ç–∞–≤–ª—è—Ç—å —Å–≤–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏. –ï—Å–ª–∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—à—å –ø—Ä–æ–µ–∫—Ç –Ω–∞ –≤—Ç–æ—Ä–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏ –≤—ã—à–µ, –Ω–µ –∑–∞–±—ã–≤–∞–π –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–æ–º–µ—Ä –∏—Ç–µ—Ä–∞—Ü–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞ v.2\".\n",
    "</div> \n",
    "\n",
    "<br/>    \n",
    "    \n",
    "–£–≤–∏–¥–µ–≤ —É —Ç–µ–±—è –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç—å, –≤ –ø–µ—Ä–≤—ã–π —Ä–∞–∑ —è –ª–∏—à—å —É–∫–∞–∂—É –Ω–∞ –µ–µ –Ω–∞–ª–∏—á–∏–µ –∏ –¥–∞–º —Ç–µ–±–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–∞–º–æ–º—É –Ω–∞–π—Ç–∏ –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –µ–µ. –ù–∞ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ç–≤–æ–π —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –±—É–¥–µ—Ç –ø–æ—Å—Ç—É–ø–∞—Ç—å —Ç–∞–∫–∂–µ, –∏ —è –ø—ã—Ç–∞—é—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ç–µ–±—è –∏–º–µ–Ω–Ω–æ –∫ —Ä–∞–±–æ—Ç–µ –¥–∞—Ç–∞—Å–∞–µ–Ω—Ç–∏—Å—Ç–æ–º. –ù–æ –µ—Å–ª–∏ —Ç—ã –ø–æ–∫–∞ –Ω–µ —Å–ø—Ä–∞–≤–∏—à—å—Å—è —Å —Ç–∞–∫–æ–π –∑–∞–¥–∞—á–µ–π - –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–µ —è –¥–∞–º –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É!ü§ì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞\" data-toc-modified-id=\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞</a></span></li><li><span><a href=\"#–û–±—É—á–µ–Ω–∏–µ\" data-toc-modified-id=\"–û–±—É—á–µ–Ω–∏–µ-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>–û–±—É—á–µ–Ω–∏–µ</a></span></li><li><span><a href=\"#–í—ã–≤–æ–¥—ã\" data-toc-modified-id=\"–í—ã–≤–æ–¥—ã-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>–í—ã–≤–æ–¥—ã</a></span></li><li><span><a href=\"#–ß–µ–∫-–ª–∏—Å—Ç-–ø—Ä–æ–≤–µ—Ä–∫–∏\" data-toc-modified-id=\"–ß–µ–∫-–ª–∏—Å—Ç-–ø—Ä–æ–≤–µ—Ä–∫–∏-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>–ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–µ–∫—Ç –¥–ª—è ¬´–í–∏–∫–∏—à–æ–ø¬ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞:</b> –ü—Ä–∏–≤–µ—Ç. –ú–µ–Ω—è –∑–æ–≤—É—Ç –ö–∞–ª–∞–º–∫–∞—Å, –º–æ–∂–Ω–æ –ö–∞—Å—è. –Ø –ø—ã—Ç–∞—é—Å—å –æ–±—É—á–∏—Ç—å 2 –º–æ–¥–µ–ª–∏ –°–õ –∏ –õ–† –Ω–∞ –ø–æ–ª—É—á–∏–≤—à–∏—Ö—Å—è —Ñ–∏—á–∞—Ö - –æ–∫–æ–ª–æ 190–∫ —à—Ç, —É –º–µ–Ω—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø–∞–¥–∞–µ—Ç —è–¥—Ä–æ, —Ö–æ—Ç—è —Ä–∞–±–æ—Ç–∞—é —è –Ω–∞ –ª–æ–∫–∞–ª–µ. –Ø —É–∂–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –°–õ - —É–∫–∞–∑–∞–≤ –∫–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ = –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å –∏–∑ –æ–±—â–µ–≥–æ –∫–æ–ª-–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–∞–∫ –Ω–∞ —Ö–∞–±—Ä–µ –ø–∏—Å–∞–ª–∏ —á—Ç–æ –≤—Ä–æ–¥–µ –∫–∞–∫ –¥–∞–∂–µ –ø–æ –¥—ç—Ñ–æ–ª—Ç—É —Å–∞–º–æ–π –ø—Ä–æ–≥—Ä–∞–º–º–æ–π —Ç–∞–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Ä–∞–∑–±–∏–µ–Ω–∏–∏), –∏ —É–∫–∞–∑—ã–≤–∞–ª–∞ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã - –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ü—ã—Ç–∞–ª–∞—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –º–µ–Ω—å—à–µ–π –≤—ã–±–æ—Ä–∫–µ - –≤—Å–µ –æ–∫, –¥—É–º–∞—é —Ç—É—Ç –Ω–∞–¥–æ –∫–∞–∫—Ç–æ –µ–µ –±–∏—Ç—å –Ω–∞ –±–∞—Ç—á–∏? –ù–æ –Ω–µ —Å–æ–≤—Å–µ–º –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å, –º–± –ø–æ–¥—Å–∫–∞–∂–µ—à—å —Ç—É—Ç –ª–∏–±–æ —Å—Ç–∞—Ç–µ–π–∫—É –∫–∞–∫—É—é –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—à—å? –ë—É–¥—É –æ—á–µ–Ω—å –ø—Ä–∏–∑–Ω–∞—Ç–µ–ª—å–Ω–∞.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞  <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>\n",
    "    \n",
    "–ü—Ä–æ–±–ª–µ–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–∞ —Å —Ç–µ–º, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–∞–¥–µ–Ω–∏—é —è–¥—Ä–∞.\n",
    "    \n",
    "–û–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã - –ø—Ä–æ–≤–µ—Å—Ç–∏ –æ—á–∏—Å—Ç–∫—É –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é, –∞ —Ä–∞–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤. \n",
    "    \n",
    "–î—Ä—É–≥–æ–π –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (PCA). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∏—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å. –¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—ã–±—Ä–∞–Ω—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏, –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∏, –≤–æ–∑–º–æ–∂–Ω–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–í–∞–∂–Ω–æ —Ç–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –≤–æ–∑–º–æ–∂–Ω–æ –ª–∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –±–∞—Ç—á–∞—Ö, –∏ –∫–∞–∫–æ–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–ª—è –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã–±—Ä–∞—Ç—å –Ω–∞–∏–ª—É—á—à—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏.\n",
    "    \n",
    "–ù—É–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –≤—ã–±–æ—Ä–∫—É –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é  + –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏) \n",
    "    \n",
    "    \n",
    "    \n",
    "–ï—Å–ª–∏ —Ö–æ—á–µ—à—å –ª—É—á—à–µ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ —Ç–µ–º–µ, —Ç–æ –º–æ–≥—É –ø–æ—Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å —Ç–µ–±–µ: \n",
    "    \n",
    "https://huggingface.co/transformers/model_doc/bert.html\n",
    "    \n",
    "https://t.me/renat_alimbekov\n",
    "    \n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/ - –ü—Ä–æ LSTM\n",
    "    \n",
    "https://web.stanford.edu/~jurafsky/slp3/10.pdf - –ø—Ä–æ —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä –º–æ–¥–µ–ª–∏, —ç—Ç–µ–Ω—à–µ–Ω—ã\n",
    "    \n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html - –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –≥–∞–π–¥ –ø–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—É –æ—Ç —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π pytorch\n",
    "    \n",
    "https://transformer.huggingface.co/ - –ø–æ–±–æ–ª—Ç–∞—Ç—å —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º\n",
    "    \n",
    "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏: allennlp, fairseq, transformers, tensorflow-text ‚Äî –º–Ω–æ–∂–µ—Å—Ç–≤–æ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–≤ NLP\n",
    "    \n",
    "Word2Vec https://radimrehurek.com/gensim/models/word2vec.html\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω ¬´–í–∏–∫–∏—à–æ–ø¬ª –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å. –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∫–∞–∫ –≤ –≤–∏–∫–∏-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö. –¢–æ –µ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö. –ú–∞–≥–∞–∑–∏–Ω—É –Ω—É–∂–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏—Ö –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é. \n",
    "\n",
    "–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ. –í –≤–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –æ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–∫.\n",
    "\n",
    "–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –º–æ–¥–µ–ª—å —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ *F1* –Ω–µ –º–µ–Ω—å—à–µ 0.75. \n",
    "\n",
    "**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞**\n",
    "\n",
    "1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "2. –û–±—É—á–∏—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏. \n",
    "3. –°–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥—ã.\n",
    "\n",
    "–î–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å *BERT* –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å.\n",
    "\n",
    "**–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**\n",
    "\n",
    "–î–∞–Ω–Ω—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ñ–∞–π–ª–µ `toxic_comments.csv`. –°—Ç–æ–ª–±–µ—Ü *text* –≤ –Ω—ë–º —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è, –∞ *toxic* ‚Äî —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b> \n",
    "    \n",
    "–í–∏–∂—É —Ç–≤–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞. –ú–æ–ª–æ–¥–µ—Ü! –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Ç–µ–±–µ —Ä–∞—Å—Å—Ç–∞–≤–ª—è—Ç—å –∞–∫—Ü–µ–Ω—Ç—ã –≤ –≤—ã–≤–æ–¥–∞—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tqdm as notebook_tqdm\n",
    "#from nltk import WordNetLemmatizer\n",
    "#from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "#import tqdm as notebook_tqdm\n",
    "#from deeppavlov.core.common.file import read_json\n",
    "#from deeppavlov import build_model, configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>159446</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>159447</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>159448</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>159449</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>159450</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  toxic\n",
       "0                0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1                1  D'aww! He matches this background colour I'm s...      0\n",
       "2                2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3                3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4                4  You, sir, are my hero. Any chance you remember...      0\n",
       "...            ...                                                ...    ...\n",
       "159287      159446  \":::::And for the second time of asking, when ...      0\n",
       "159288      159447  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159289      159448  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159290      159449  And it looks like it was actually you who put ...      0\n",
       "159291      159450  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159292 rows x 3 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/kalamkas/Downloads/toxic_comments.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>\n",
    "    \n",
    "<b>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏üí°:</b>\n",
    "        \n",
    "–†–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–≤–µ—Å—Ç–∏ –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ (–≤–∫–ª—é—á–∞—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É .lower()), —É–±—Ä–∞—Ç—å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —é–Ω–∏–∫–æ–¥—É        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done\n",
    "–ê –ø–æ—á–µ–º—É –Ω—É–∂–Ω–æ –±—ã–ª–æ —É–±—Ä–∞—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É U?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b> –û–Ω–∞ –Ω–µ –Ω—É–∂–Ω–∞ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ä–∞–≤–¥–∞–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>\n",
    "    \n",
    "<b>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏üí°:</b>\n",
    "    \n",
    "–¢—É—Ç —Å—Ç–æ–∏—Ç –µ—â–µ –ø—Ä–æ–≤–µ—Å—Ç–∏ –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ () –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é. –ó–∞—Ä–∞–Ω–µ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂—É —Ç–µ–±—è, —á—Ç–æ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä Mystem –ø–æ–¥—Ö–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –í–º–µ—Å—Ç–æ –Ω–µ–≥–æ –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä WordNetLemmatizer() (+POS —Ç–µ–≥). \n",
    "    \n",
    "–ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –∏–∑ —Ç—Ä–µ–Ω–∞–∂–µ—Ä–∞. –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é –º—ã –¥–æ–ª–∂–Ω—ã –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É (—Å–ª–æ–≤—É –≤ —Ç–µ–∫—Å—Ç–µ).\n",
    "\n",
    "\n",
    "–î–µ–ª—é—Å—å —Å —Ç–æ–±–æ–π –ø–æ–ª–µ–∑–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏:\n",
    "        \n",
    "  \n",
    " https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
    "        \n",
    "–•–æ—á—É –æ–±—Ä–∞—Ç–∏—Ç—å —Ç–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –∫–æ–¥ –∏–∑ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ª—å—à–µ, —á–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –≤–æ—Ç –∏–∑ —ç—Ç–æ–≥–æ —Ç–æ–ø–∏–∫–∞\n",
    "    \n",
    "https://stackoverflow.com/questions/50992974/nltk-wordnetlemmatizer-not-lemmatizing-as-expected    \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"></h2>\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b> \n",
    "    \n",
    "–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å pos-—Ç–µ–≥ \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>\n",
    "    \n",
    "<b>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏üí°:</b>  \n",
    "    \n",
    "–ù–∞–ø–æ–º–∏–Ω–∞—é –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^a-zA-Z\\']', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.split())\n",
    "#data['text'] = data['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kalamkas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kalamkas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kalamkas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "wnl = WordNetLemmatizer()\n",
    "#data['text'] = data['text'].apply(lambda x: wnl.lemmatize(x))\n",
    "#data['text'] = data['text'].apply(lambda x: ''.join(x))\n",
    "#data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nh/dtychn4n2gz8tcfythvr7wthp1qbsk/T/ipykernel_1148/534125236.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'][i] = lemmatized_row\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data['text'].loc[:100])):\n",
    "    lemmatized_row = []\n",
    "    for j in range(len(nltk.pos_tag(data['text'][i]))):\n",
    "        lemmatized_row.append(wnl.lemmatize(data['text'][i][j],  get_wordnet_pos(nltk.pos_tag(data['text'][i])[j][1])))\n",
    "    data['text'][i] = lemmatized_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞:</b> –ü–æ–¥—Å–∫–∞–∂–∏ –ø–ª–∏–∑ –ø–æ—á–µ–º—É –Ω–µ —É–¥–∞–µ—Ç—Å—è –∑–∞–≤–µ—Ä–Ω—É—Ç—å —Ü–∏–∫–ª –≤ —Ñ—É–Ω–∫—Ü–∏—é (–≤—ã–¥–∞–µ—Ç –æ—à–∏–±–∫—É —è—á–µ–π–∫–æ–π –Ω–∏–∂–µ)? + –ï—Å–ª–∏ –Ω–µ —Å–ª–æ–∂–Ω–æ, –º–æ–∂–µ—à—å –ø–æ–¥—Å–∫–∞–∑–∞—Ç—å, –∫–∞–∫ —ç—Ç—É –∂–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –ø—Ä–æ–¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é –ª—è–º–±–¥–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –∫–∞–∫ —è –¥–µ–ª–∞–ª–∞ —Å lower() –∏–ª–∏ split() - —è —á–µ—Ç–æ –∑–∞—Å—Ç—Ä—è–ª–∞ –≤ —ç—Ç–æ–º, —Ç–∞–∫ —á—Ç–æ –ø–µ—Ä–µ—à–ª–∞ –≤ –∏—Ç–æ–≥–µ –Ω–∞ –Ω–µ —Å–∞–º—ã–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Å —Ü–∏–∫–ª–æ–º... </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def lemmatize(data):\n",
    "#    for i in range(len(data['text'])):\n",
    "#        lemmatized_row = []\n",
    "#        for j in range(len(nltk.pos_tag(data['text'][i]))):\n",
    "#            lemmatized_row.append(wnl.lemmatize(data['text'][i][j], get_wordnet_pos(nltk.pos_tag(data['text'][i])[j][1])))\n",
    "#        return lemmatized_row\n",
    "\n",
    "#data['text'] = data.apply(lemmatize, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞–≤–∏–ª–∞ –≤ —Ç–µ–∫—Å—Ç–∞—Ö —Ç–æ–∫ –±—É–∫–≤—ã –∞–Ω–≥–ª –∞–ª—Ñ–∞–≤–∏—Ç–∞ + –∞–ø–æ—Å—Ç—Ä–æ—Ñ - —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b> –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ, –¥–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–∑–æ–±—å–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏, –æ–±—É—á–∏–º TfidfVectorizer() –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ, –∞ –∑–∞—Ç–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏–º –æ–±–µ –≤—ã–±–æ—Ä–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<s>–ù–æ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤ –≤ –ø–æ–ª–µ —Å —Ü–µ–ª–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º.</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_frequency = data['toxic'].value_counts(normalize=True)\n",
    "#print(class_frequency)\n",
    "#class_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<s>–î–∏—Åc–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –æ—á–µ–≤–∏–¥–µ–Ω. –£–≤–µ–ª–∏—á–∏–º –∫–æ–ª-–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π –∫–ª–∞—Å—Å–∞ 1 –º–µ—Ç–æ–¥–æ–º upsampling.</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def upsample(data):\n",
    "#    data_zeros = data[data['toxic'] == 0]\n",
    "#    data_ones = data[data['toxic'] == 1]\n",
    "#    data_upsampled = pd.concat([data_zeros] + [data_ones] * int((len(data_zeros) / len(data_ones))))\n",
    "#    data_upsampled = shuffle(data_upsampled, random_state=12345)\n",
    "#    return data_upsampled\n",
    "\n",
    "#data_upsampled = upsample(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2  <a class=\"tocSkip\"></h2>\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b>\n",
    "    \n",
    "    \n",
    "–ü—Ä–æ–≤–æ–¥–∏—Ç—å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ upsampled/downsampled –¥–∞–Ω–Ω—ã—Ö ‚Äì –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –Ω–∞—Ä—É—à–µ–Ω. –ù–∞–ø–æ–º–Ω—é, —á—Ç–æ –≤–Ω—É—Ç—Ä–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–π –≤ –Ω–µ–µ –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ —Ç—Ä–µ–∏–Ω –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é. \n",
    "    \n",
    "–í —Å–ª—É—á–∞–µ upsampling –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ç–∞–∫, —á—Ç–æ –≤ —Ç—Ä–µ–∏–Ω –∏ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏—é (–≤–Ω—É—Ç—Ä–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏) –≤–æ–æ–±—â–µ –ø–æ–ø–∞–¥–∞—é—Ç –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –æ–±—ä–µ–∫—Ç—ã. \n",
    "\n",
    "–ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–¥—Ö–æ–¥ –∏–∑ —Å—Ç–∞—Ç—å–∏: https://datascience.xyz/practice/kak-delat-kross-validaciju-pri-apsemplinge-dannyh.html, –µ—â–µ –æ—Ç–º–µ—á—É, —á—Ç–æ –ø–æ —É—Å–ª–æ–≤–∏—é –∑–∞–¥–∞–Ω–∏—è –ø—Ä–æ–≤–æ–¥–∏—Ç—å upsampling/downsampling –¥–∞–Ω–Ω—ã—Ö –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø—Ä–∞–≤–∏–ª–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_frequency_upsampled = data_upsampled['toxic'].value_counts(normalize=True)\n",
    "#print(class_frequency_upsampled)\n",
    "#class_frequency_upsampled.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<s>–ï—Å—Ç—å –Ω–µ–±–æ–ª—å—à–∞—è –¥–∏—Å–ø—Ä–æ–ø–æ—Ä—Ü–∏—è –∏–∑–∑–∞ –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è len(data_zeros) / len(data_ones), –Ω–æ, –¥—É–º–∞—é, —ç—Ç–æ —É–∂–µ –Ω–µ —Ç–∞–∫ –∫—Ä–∏—Ç–∏—á–Ω–æ.</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, random_state=12345, test_size=0.25)\n",
    "#train, test = train_test_split(data_upsampled, random_state=12345, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kalamkas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=list(stop_words))\n",
    "corpus_train = train['text'].values\n",
    "corpus_test = test['text'].values\n",
    "count_tf_idf.fit(corpus_train)\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞  <a class=\"tocSkip\"></h2>\n",
    "\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b>\n",
    "    \n",
    "–ß—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —É—Ç–µ—á–∫–∏, –æ–±—É—á–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω—É–∂–Ω–æ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä–∫–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, –∑–∞—Ç–µ–º –º–µ—Ç–æ–¥ transform –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫–æ –≤—Å–µ–º –≤—ã–±–æ—Ä–∫–∞–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train, target_test = train['toxic'], test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"></h2>\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b> \n",
    "    \n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–≤–µ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ grid.best_score_ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏  –∏–ª–∏ —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"></h2>\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b>\n",
    "    \n",
    "–°–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –Ω–µ –Ω—É–∂–Ω–∞—è –Ω–∞–º –º–µ—Ç—Ä–∏–∫–∞ –≤ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏, –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏.   –°–ª–µ–¥—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ‚Äòf1‚Äô. –ü–æ–¥–∞—Ç—å –µ–µ –Ω—É–∂–Ω–æ –≤–Ω—É—Ç—Ä—å cross_val_score –∏/–∏–ª–∏ GridSearchCV, –∏—Å–ø–æ–ª—å–∑—É—è —Å–∫–æ—Ä–µ—Ä `scoring=...`. –ò—Å–ø—Ä–∞–≤—å, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞\n",
    "    \n",
    "(—ç—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ cross_val_score –∏/–∏–ª–∏ GridSearchCV)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞:</b> –í–æ—Ç —Ç—É—Ç –∏ –ø—Ä–æ–±–ª–µ–º–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç...</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞  <a class=\"tocSkip\"> </h2>\n",
    "    \n",
    "<b>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏üí°:</b>  \n",
    "    \n",
    "–ù–µ —Å–æ–≤–µ—Ç—É—é —Ç–µ–±–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ numpy array. –ú—ã –∏–º–µ–µ–º –¥–ª–∏–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∏ —Ç–∞–º –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω—É–ª–µ–π, –¥–ª—è –∏—Ö —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã, –Ω–µ —Ç—Ä–∞—Ç—è—â–∏–µ –ø–∞–º—è—Ç—å –Ω–∞ –Ω—É–ª–∏. –ï—Å–ª–∏ –∏—Ö –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –≤ –ø–ª–æ—Ç–Ω—É—é —Ñ–æ—Ä–º—É (numpy array), —Ç–æ –º–æ–∂–µ—Ç –Ω–µ —Ö–≤–∞—Ç–∏—Ç—å –ø–∞–º—è—Ç–∏    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –ø–æ–Ω—è—Ç–Ω–æ –ø–æ—á–µ–º—É —è–¥—Ä–æ –ø–∞–¥–∞–ª–æ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å –∏ –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ —Ç—Ä–µ–π–Ω–µ, –ø–µ—Ä–µ–±—Ä–∞–≤ —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1}\n",
      "Best score is: 0.1245971883884005\n",
      "--- 177.26854991912842 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rf = RandomForestClassifier()\n",
    "grid_space = {\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'n_estimators': [1, 2, 5, 10],\n",
    "#    'max_features': [435, 189229],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [1, 2, 3]\n",
    "}\n",
    "grid = GridSearchCV(rf, param_grid=grid_space, cv=3, scoring='f1')\n",
    "model_grid = grid.fit(tf_idf_train, target_train)\n",
    "print('Best hyperparameters are: '+str(model_grid.best_params_))\n",
    "print('Best score is: '+str(model_grid.best_score_))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "36 fits failed out of a total of 72.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1216, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.73086712        nan        nan 0.7602696  0.76173208 0.76175292\n",
      "        nan        nan 0.73086712        nan        nan 0.75460413\n",
      " 0.76192841 0.76182059        nan        nan 0.73086712        nan\n",
      "        nan 0.75195893 0.76027061 0.76040695        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best score is: 0.7619284131236798\n",
      "--- 57.41955900192261 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lr = LogisticRegression(random_state=12345, max_iter=100, class_weight='balanced')\n",
    "grid_space = {\n",
    "    'penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "    'C': [5, 10, 15],\n",
    "    'solver': ['lbfgs', 'liblinear'] #, 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "}\n",
    "grid = GridSearchCV(lr, param_grid=grid_space, cv=3, scoring='f1')\n",
    "model_grid = grid.fit(tf_idf_train, target_train)\n",
    "print('Best hyperparameters are: '+str(model_grid.best_params_))\n",
    "print('Best score is: '+str(model_grid.best_score_))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–∞–∫, –º–æ–¥–µ–ª—å –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø–æ–∫–∞–∑–∞–ª–∞ –æ—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - f1 = <s>0.98</s> 0.76. –í—ã–±–µ—Ä–µ–º —ç—Ç—É –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b> –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7729636048526862\n",
      "--- 2.265101194381714 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalamkas/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lr = LogisticRegression(random_state=12345, max_iter=100, class_weight='balanced', C=10, penalty='l2', solver='lbfgs')\n",
    "lr.fit(tf_idf_train, target_train)\n",
    "predicted_test = lr.predict(tf_idf_test)\n",
    "print(f1_score(target_test, predicted_test))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–Ω–∞—á–µ–Ω–∏–µ f1-–º–µ—Ä—ã –Ω–∞ —Ç–µ—Å—Ç–µ —Ç–∞–∫–∂–µ –æ–∫–∞–∑–∞–ª–æ—Å—å –≤—ã—Å–æ–∫–∏–º = 0.77 <s>0.98.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b> \n",
    "    \n",
    "–ù–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –ø–æ–ª—É—á–µ–Ω–æ —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –º—ã –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ - –ø—Ä–æ–≤–µ–ª–∏ –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é, –∑–∞—Ç–µ–º –ø–µ—Ä–µ–≤–µ–ª–∏ —Ç–µ–∫—Å—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç, –∏ —É–∂–µ –Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∏–ª–∏ 2 —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–¥–æ–±—Ä–∞–≤ –Ω—É–∂–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å —Ç–∑ f1-–º–µ—Ä—ã. –õ—É—á—à—É—é –º–æ–¥–µ–ª—å - –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é - —Ç–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–µ, f1 —Å–æ—Å—Ç–∞–≤–∏–ª–æ 0.77 <s>0.98</s>, —á—Ç–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç –Ω–∞—à–µ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é f1 > 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"></h2>\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b>\n",
    "    \n",
    "–°–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –Ω–µ –Ω—É–∂–Ω–∞—è –Ω–∞–º –º–µ—Ç—Ä–∏–∫–∞ —Å–ª–µ–¥—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ‚Äòf1‚Äô."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>\n",
    "    \n",
    "–õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å + –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –° - —ç—Ç–æ —Å–µ–∫—Ä–µ—Ç —É—Å–ø–µ—Ö–∞ –≤ —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ. –ü–æ—ç—Ç–æ–º—É –µ—Å—Ç—å —Å–º—ã—Å–ª  –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π (–ø–µ–Ω–∞–ª—Ç–∏ l1/l2 + –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –° –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 5-15). –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–∏–ª–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏) —Ä–∞—Å–∫—Ä—ã—Ç—å—Å—è –≤ –ø–æ–ª–Ω—É—é —Å–∏–ª—É\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"></h2>\n",
    "    \n",
    "<b>–ù–∞ –¥–æ—Ä–∞–±–æ—Ç–∫—Éü§î:</b> \n",
    "    \n",
    "–ù–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω—É–∂–Ω–æ –∏–∑–º–µ—Ä–∏—Ç—å —Ç–æ–ª—å–∫–æ –û–î–ù–£ ‚Äì –ª—É—á—à—É—é –º–æ–¥–µ–ª—å. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–≤–µ—Å—Ç–∏ –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>\n",
    "\n",
    "<b>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ!üëç:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:solid Chocolate 2px; padding: 40px\">\n",
    "\n",
    "\n",
    "<h2> –ò—Ç–æ–≥–æ–≤—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ <a class=\"tocSkip\"> </h2>    \n",
    "\n",
    "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø—Ä–æ–µ–∫—Ç –Ω–µ –≥–æ—Ç–æ–≤. –ü—Ä–∏—Å—ã–ª–∞–π –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é, —Å–µ–π—á–∞—Å –≤ –ø—Ä–æ–µ–∫—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —à–∞–≥–∏ –ø–æ –∑–∞–¥–∞–Ω–∏—è–º. \n",
    "    \n",
    "–°—Ç–æ–∏—Ç –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∑–∞ –ø–æ–º–æ—â—å—é –∫ –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞–º –∏ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è–º, –∞ —Ç–∞–∫–∂–µ –∫–æ–º—å—é–Ω–∏—Ç–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤.\n",
    "   \n",
    "    \n",
    "–ñ–¥—É –æ—Ç —Ç–µ–±—è –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –ø—Ä–æ–µ–∫—Ç–∞. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:solid Chocolate 2px; padding: 40px\">\n",
    "\n",
    "\n",
    "<h2> –ò—Ç–æ–≥–æ–≤—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ 2 <a class=\"tocSkip\"> </h2>    \n",
    "    \n",
    "  \n",
    "–£ –º–µ–Ω—è —Å–ª–æ–∂–∏–ª–æ—Å—å —Ö–æ—Ä–æ—à–µ–µ –æ–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ –æ –ø—Ä–æ–µ–∫—Ç–µ, —Ç–µ–±–µ —É–¥–∞–ª–æ—Å—å –Ω–µ–ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–∏–º –ø—Ä–æ–µ–∫—Ç–æ–º. –ú–æ–ª–æ–¥–µ—Ü! –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –∏–∑—É—á–µ–Ω –∫–∞–∂–¥—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä. –ü–ø–æ—Å—Ç—Ä–æ–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –æ—Ü–µ–Ω–µ–Ω–æ –∏—Ö –∫–∞—á–µ—Å—Ç–≤–æ. –û—Å–º—ã—Å–ª–µ–Ω–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ - –º–Ω–æ–≥–æ–µ —É–¥–∞–ª–æ—Å—å –∫–∞–∫ –Ω–∞–¥–æ)\n",
    "    \n",
    "–û—Ç–º–µ—á—É –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞üôÇ:\n",
    "    \n",
    "- –≤ —Ö–æ–¥–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å  —Ñ—É–Ω–∫—Ü–∏–∏, –ø–æ–º–æ–≥–∞—é—â–∏–µ –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞;\n",
    "- —Ö–æ—Ä–æ—à–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è;\n",
    "- –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "    \n",
    "–ï—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–º–µ–Ω—Ç–æ–≤ –≤—Å–µ–≥–æ, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ–∏—Ç –µ—â—ë —Ä–∞–∑ –≤–∑–≥–ª—è–Ω—É—Ç—å, —è —É–∫–∞–∑–∞–ª –∏—Ö –≤ –º–æ–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –ø–æ —Ö–æ–¥—É –ø—Ä–æ–µ–∫—Ç–∞. –ü—Ä–µ–¥–ª–∞–≥–∞—é —Ç–µ–±–µ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–µ–∫—Ç –ø–æ –º–æ–∏–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º, —á—Ç–æ–±—ã –¥–æ–≤–µ—Å—Ç–∏ –µ–≥–æ –¥–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–∞.\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "–ï—Å–ª–∏ –±—É–¥—É—Ç –≤–æ–ø—Ä–æ—Å—ã, –æ–±—Ä–∞—â–∞–π—Å—è, —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –Ω–∞ –Ω–∏—Ö –æ—Ç–≤–µ—á—É.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Ä–æ–¥–µ –≤—Å–µ –ø–æ–ø—Ä–∞–≤–∏–ª–∞. –°–ø–∞—Å–∏–±–æ –∑–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression(random_state=12345, solver='lbfgs', max_iter=20)\n",
    "#accuracy = cross_val_score(model, features, target, cv=20, scoring='accuracy', n_jobs=-1, error_score='raise').mean()\n",
    "#print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.BertTokenizer.from_pretrained('distilbert-base-uncased') #(vocab_file='/datasets/ds_bert/vocab.txt')\n",
    "#tokenized = data['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len = 0\n",
    "#for i in tokenized.values:\n",
    "#    if len(i) < max_len:\n",
    "#        max_len = len(i)\n",
    "#padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])\n",
    "#attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = transformers.BertConfig.from_json_file('/Users/kalamkas/Desktop/bert_embedder.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook –æ—Ç–∫—Ä—ã—Ç\n",
    "- [ ]  –í–µ—Å—å –∫–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫\n",
    "- [ ]  –Ø—á–µ–π–∫–∏ —Å –∫–æ–¥–æ–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –≤ –ø–æ—Ä—è–¥–∫–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "- [ ]  –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã\n",
    "- [ ]  –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã\n",
    "- [ ]  –ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ *F1* –Ω–µ –º–µ–Ω—å—à–µ 0.75\n",
    "- [ ]  –í—ã–≤–æ–¥—ã –Ω–∞–ø–∏—Å–∞–Ω—ã"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 358,
    "start_time": "2023-06-26T07:21:59.956Z"
   },
   {
    "duration": 2338,
    "start_time": "2023-06-26T07:22:15.903Z"
   },
   {
    "duration": 798,
    "start_time": "2023-06-26T07:22:58.397Z"
   },
   {
    "duration": 839,
    "start_time": "2023-06-26T07:23:02.252Z"
   },
   {
    "duration": 1627,
    "start_time": "2023-06-26T07:25:02.413Z"
   },
   {
    "duration": 3897,
    "start_time": "2023-06-26T07:25:10.367Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-27T08:54:33.714Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-27T08:55:36.343Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-27T08:55:36.758Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-27T08:55:37.831Z"
   },
   {
    "duration": 2046,
    "start_time": "2023-06-27T18:18:49.241Z"
   },
   {
    "duration": 2495,
    "start_time": "2023-06-27T18:18:51.289Z"
   },
   {
    "duration": 144,
    "start_time": "2023-06-27T18:19:27.362Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-27T18:19:40.054Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-27T18:19:58.311Z"
   },
   {
    "duration": 469,
    "start_time": "2023-06-27T18:22:01.631Z"
   },
   {
    "duration": 383,
    "start_time": "2023-06-27T18:22:17.812Z"
   },
   {
    "duration": 318,
    "start_time": "2023-06-27T18:23:37.341Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-27T18:24:19.614Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-27T18:24:39.610Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-27T18:24:54.698Z"
   },
   {
    "duration": 537,
    "start_time": "2023-06-27T18:25:34.602Z"
   },
   {
    "duration": 339,
    "start_time": "2023-06-27T18:26:14.384Z"
   },
   {
    "duration": 364,
    "start_time": "2023-06-27T18:26:39.112Z"
   },
   {
    "duration": 23,
    "start_time": "2023-06-27T18:27:25.235Z"
   },
   {
    "duration": 1249,
    "start_time": "2023-06-27T18:27:36.190Z"
   },
   {
    "duration": 1224,
    "start_time": "2023-06-27T18:27:58.683Z"
   },
   {
    "duration": 1137,
    "start_time": "2023-06-27T18:28:30.772Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-27T18:29:03.286Z"
   },
   {
    "duration": 1159,
    "start_time": "2023-06-27T18:29:16.262Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-27T18:29:49.830Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-27T18:29:53.928Z"
   },
   {
    "duration": 899,
    "start_time": "2023-06-27T18:29:53.936Z"
   },
   {
    "duration": 16,
    "start_time": "2023-06-27T18:29:54.837Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-27T18:30:02.721Z"
   },
   {
    "duration": 861,
    "start_time": "2023-06-27T18:30:02.728Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-27T18:30:09.444Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
